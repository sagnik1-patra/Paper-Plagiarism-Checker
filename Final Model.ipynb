{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50bccbd8-50e4-4691-a629-8b15ee42396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå File not found: your_paper.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Make sure to download punkt (only once)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# === PART 1: Read Text File ===\n",
    "def read_text_file(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"‚ùå File not found:\", file_path)\n",
    "        return None\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# === PART 2: Sentence Tokenization ===\n",
    "def get_sentences(text):\n",
    "    try:\n",
    "        return sent_tokenize(text)\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "# === PART 3: Plagiarism Checker (TF-IDF + Cosine Similarity) ===\n",
    "def check_plagiarism(sentences, threshold=0.8):\n",
    "    tfidf = TfidfVectorizer().fit_transform(sentences)\n",
    "    cosine_sim = cosine_similarity(tfidf, tfidf)\n",
    "    \n",
    "    n = len(sentences)\n",
    "    plagiarized_pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if cosine_sim[i][j] >= threshold:\n",
    "                plagiarized_pairs.append((i, j, cosine_sim[i][j]))\n",
    "    \n",
    "    return plagiarized_pairs\n",
    "\n",
    "# === PART 4: Accuracy vs Iterations Graph ===\n",
    "def plot_accuracy_graph():\n",
    "    iterations = np.arange(1, 11)\n",
    "    accuracy = np.random.uniform(0.7, 0.95, size=10)  # Dummy accuracy values\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(iterations, accuracy, marker='o')\n",
    "    plt.title(\"Accuracy vs Iterations\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"accuracy_graph.png\")\n",
    "    plt.show()\n",
    "\n",
    "# === PART 5: Confusion Matrix Heatmap (Dummy Data) ===\n",
    "def plot_confusion_matrix():\n",
    "    y_true = [0, 1, 2, 2, 0, 1, 2, 0, 1, 1]\n",
    "    y_pred = [0, 2, 2, 2, 0, 0, 2, 0, 1, 1]\n",
    "    labels = [0, 1, 2]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix Heatmap')\n",
    "    plt.savefig(\"confusion_matrix_heatmap.png\")\n",
    "    plt.show()\n",
    "\n",
    "# === MAIN DRIVER ===\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"your_paper.txt\"  # Change this to your paper path\n",
    "    \n",
    "    text = read_text_file(file_path)\n",
    "    if text:\n",
    "        sentences = get_sentences(text)\n",
    "        \n",
    "        if len(sentences) > 1:\n",
    "            plagiarized = check_plagiarism(sentences)\n",
    "            \n",
    "            if plagiarized:\n",
    "                print(\"üîç Plagiarized Sentence Pairs:\")\n",
    "                for i, j, sim in plagiarized:\n",
    "                    print(f\"Sentence {i} & {j} - Similarity: {sim:.2f}\")\n",
    "            else:\n",
    "                print(\"‚úÖ No plagiarized pairs found.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Not enough sentences for plagiarism check.\")\n",
    "        \n",
    "        # Plot graphs\n",
    "        plot_accuracy_graph()\n",
    "        plot_confusion_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8952405-5e1d-4232-be7c-1c472197a759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Collecting googlesearch-python\n",
      "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: click in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from googlesearch-python) (4.13.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from googlesearch-python) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.20->googlesearch-python) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: googlesearch-python\n",
      "Successfully installed googlesearch-python-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 nltk googlesearch-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51674ced-2dcc-4c4f-9176-66ad859a3bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Reading PDF: C:\\Users\\sagni\\Downloads\\Band Selection\\paper\\IIIT Paper.pdf\n",
      "üìù Total sentences extracted: 113\n",
      "üîç Checking for similar (possibly plagiarized) sentences...\n",
      "‚úÖ No plagiarism detected above threshold.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load PDF and extract text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error reading PDF:\", e)\n",
    "        return \"\"\n",
    "    return text\n",
    "\n",
    "# Split text into sentences using regex\n",
    "def split_into_sentences(text):\n",
    "    # This regex splits on punctuation followed by space and a capital letter (basic approximation)\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+(?=[A-Z])', text)\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]  # Filter short sentences\n",
    "    return sentences\n",
    "\n",
    "# Plagiarism check function using cosine similarity\n",
    "def check_plagiarism(sentences, threshold=0.8):\n",
    "    tfidf_vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
    "    similarity_matrix = cosine_similarity(tfidf_vectorizer)\n",
    "\n",
    "    plagiarized_pairs = []\n",
    "    for i in range(len(similarity_matrix)):\n",
    "        for j in range(i + 1, len(similarity_matrix)):\n",
    "            similarity_score = similarity_matrix[i][j]\n",
    "            if similarity_score >= threshold:\n",
    "                plagiarized_pairs.append((i, j, similarity_score))\n",
    "    \n",
    "    return plagiarized_pairs\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    pdf_path = r\"C:\\Users\\sagni\\Downloads\\Band Selection\\paper\\IIIT Paper.pdf\"\n",
    "    print(\"üìÑ Reading PDF:\", pdf_path)\n",
    "\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    if not text:\n",
    "        print(\"‚ö†Ô∏è No text extracted from the PDF.\")\n",
    "        return\n",
    "\n",
    "    sentences = split_into_sentences(text)\n",
    "    print(f\"üìù Total sentences extracted: {len(sentences)}\")\n",
    "\n",
    "    if len(sentences) < 2:\n",
    "        print(\"‚ö†Ô∏è Not enough sentences to compare for plagiarism.\")\n",
    "        return\n",
    "\n",
    "    print(\"üîç Checking for similar (possibly plagiarized) sentences...\")\n",
    "    results = check_plagiarism(sentences)\n",
    "\n",
    "    if not results:\n",
    "        print(\"‚úÖ No plagiarism detected above threshold.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Detected {len(results)} possibly plagiarized sentence pairs:\")\n",
    "        for i, j, score in results:\n",
    "            print(f\"\\nüîÅ Similarity Score: {score:.2f}\")\n",
    "            print(f\"Sentence {i + 1}: {sentences[i]}\")\n",
    "            print(f\"Sentence {j + 1}: {sentences[j]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d342b23-5fa7-426d-8460-c4e08d260c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
